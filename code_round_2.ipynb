{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script:\n",
    "#   - Checks if CUDA-enabled GPUs are available.\n",
    "#   - Configures Spark for 6 cores and ~10GB total (5g each for driver and executor).\n",
    "#   - If GPUs are available and your environment supports RAPIDS, extra GPU configurations are applied.\n",
    "#   - Ingests and filters a massive CSV file using a defined schema.\n",
    "#   - Performs anomaly filtering (keeps rows where soil_moisture is within ±3 standard deviations).\n",
    "#   - Sorts data by multiple columns and applies further filtering.\n",
    "#   - Computes descriptive statistics (mean, median, stddev, Q1, Q3) for carbon_percent and nitrogen_percent.\n",
    "#   - Interpolates missing carbon_percent values (if null, sets value to soil_moisture * 0.05).\n",
    "#   - Builds and tests a Linear Regression model (using Spark ML) to predict carbon_percent from soil_moisture and temperature.\n",
    "#   - Generates a bar chart (in a separate thread) of the nutrient statistics.\n",
    "#   - Reports total runtime.\n",
    "  \n",
    "# Note: To truly leverage GPUs make sure your Spark cluster is configured for GPU acceleration (e.g., with RAPIDS Accelerator) and that the necessary drivers and discovery scripts are available.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from threading import Thread\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, percentile_approx, when\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for CUDA-enabled GPUs.\n",
    "def has_cuda():\n",
    "    try:\n",
    "        import torch\n",
    "        return torch.cuda.is_available()\n",
    "    except ImportError:\n",
    "        try:\n",
    "            result = subprocess.run([\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            return result.returncode == 0\n",
    "        except Exception:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA cores detected. Running on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Determine compute mode.\n",
    "use_gpu = has_cuda()\n",
    "if use_gpu:\n",
    "    print(\"CUDA cores detected. Configuring Spark for GPU acceleration.\")\n",
    "else:\n",
    "    print(\"No CUDA cores detected. Running on CPU.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created.\n"
     ]
    }
   ],
   "source": [
    "# Start timing.\n",
    "start_time = time.time()\n",
    "\n",
    "# Define schema for the CSV.\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", IntegerType(), True),\n",
    "    StructField(\"soil_moisture\", FloatType(), True),\n",
    "    StructField(\"soil_water_content\", FloatType(), True),\n",
    "    StructField(\"carbon_percent\", FloatType(), True),\n",
    "    StructField(\"nitrogen_percent\", FloatType(), True),\n",
    "    StructField(\"atmospheric_humidity\", FloatType(), True),\n",
    "    StructField(\"temperature\", FloatType(), True),\n",
    "    StructField(\"pH\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create Spark session with resource settings.\n",
    "spark_builder = SparkSession.builder \\\n",
    "    .appName(\"1BRC_Round2\") \\\n",
    "    .master(\"local[6]\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.local.dir\", \"D:/spark_temp/\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"20\") \\\n",
    "    .config(\"spark.default.parallelism\", \"12\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "if use_gpu:\n",
    "    # These configurations are examples; proper GPU use requires a GPU-enabled Spark setup.\n",
    "    spark_builder = spark_builder \\\n",
    "        .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "        .config(\"spark.executor.resource.gpu.amount\", \"1\") \\\n",
    "        .config(\"spark.task.resource.gpu.amount\", \"0.5\") \\\n",
    "        .config(\"spark.executor.resource.gpu.discoveryScript\", \"/path/to/gpuDiscoveryScript.sh\")\n",
    "\n",
    "spark = spark_builder.getOrCreate()\n",
    "print(\"Spark session created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: Data Ingestion & Preparation\n",
      "Data loaded with initial filtering.\n",
      "Estimated total rows: ~798410000\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Data Ingestion & Preparation.\n",
    "print(\"Task 1: Data Ingestion & Preparation\")\n",
    "file_path = \"combined.csv\"  # Update with your CSV file location.\n",
    "try:\n",
    "    df = spark.read.schema(schema).csv(file_path, header=True) \\\n",
    "            .filter(\n",
    "                (col(\"soil_moisture\").between(0, 100)) &\n",
    "                (col(\"soil_water_content\").between(0, 100)) &\n",
    "                (col(\"carbon_percent\").between(0, 10)) &\n",
    "                (col(\"nitrogen_percent\").between(0, 5)) &\n",
    "                (col(\"atmospheric_humidity\").between(0, 100)) &\n",
    "                (col(\"temperature\").between(0, 40)) &\n",
    "                (col(\"pH\").between(4, 9))\n",
    "            )\n",
    "    print(\"Data loaded with initial filtering.\")\n",
    "    sample_df = df.sample(fraction=0.0001, seed=42)\n",
    "    sample_count = sample_df.count()\n",
    "    estimated_rows = int(sample_count / 0.0001)\n",
    "    print(f\"Estimated total rows: ~{estimated_rows}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Cleaning Data (Anomaly Filtering Only)\n",
      "Sample of anomaly-filtered data:\n",
      "+---------+-------------+------------------+--------------+----------------+--------------------+-----------+----+\n",
      "|timestamp|soil_moisture|soil_water_content|carbon_percent|nitrogen_percent|atmospheric_humidity|temperature|  pH|\n",
      "+---------+-------------+------------------+--------------+----------------+--------------------+-----------+----+\n",
      "|        2|          1.5|             59.92|          8.34|            3.59|               53.55|       9.82|8.01|\n",
      "|        3|        59.39|              82.4|          2.05|            2.71|               67.54|      39.78|5.03|\n",
      "|        5|        54.58|             77.19|          5.34|            3.32|               30.31|       32.0|8.44|\n",
      "|        6|        66.24|             12.15|          1.49|            2.69|                3.43|       1.22|8.07|\n",
      "|        7|        81.44|             60.34|          6.56|            0.62|               10.97|      15.28|5.75|\n",
      "+---------+-------------+------------------+--------------+----------------+--------------------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Cleaning – Anomaly Filtering Only.\n",
    "print(\"Task 2: Cleaning Data (Anomaly Filtering Only)\")\n",
    "stats_row = df.sample(fraction=0.001, seed=42) \\\n",
    "              .select(mean(\"soil_moisture\").alias(\"mean_moisture\"),\n",
    "                      stddev(\"soil_moisture\").alias(\"stddev_moisture\")\n",
    "              ).collect()[0]\n",
    "mean_moisture = stats_row[\"mean_moisture\"]\n",
    "stddev_moisture = stats_row[\"stddev_moisture\"]\n",
    "df_clean = df.filter(((col(\"soil_moisture\") - mean_moisture) / stddev_moisture).between(-3, 3))\n",
    "print(\"Sample of anomaly-filtered data:\")\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: Multi-Column Sorting & Filtering\n",
      "Sorted sample:\n",
      "+---------+-------------+------------------+--------------+----------------+--------------------+-----------+----+\n",
      "|timestamp|soil_moisture|soil_water_content|carbon_percent|nitrogen_percent|atmospheric_humidity|temperature|  pH|\n",
      "+---------+-------------+------------------+--------------+----------------+--------------------+-----------+----+\n",
      "|    61166|        66.19|              26.1|          7.78|            0.41|               90.57|      37.13|4.57|\n",
      "|    68508|        38.12|              27.2|          3.44|            3.71|               57.62|       6.94|7.44|\n",
      "|    74499|        53.46|              7.67|          3.25|            0.69|               76.61|      38.44| 4.7|\n",
      "|    75698|        29.86|              57.8|          6.87|            2.67|               47.53|      37.49|8.18|\n",
      "|    82680|        44.08|             75.92|          9.57|            1.43|               91.44|      24.54| 7.9|\n",
      "+---------+-------------+------------------+--------------+----------------+--------------------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Filtered sample:\n",
      "+---------+-------------+------------------+--------------+----------------+--------------------+-----------+----+\n",
      "|timestamp|soil_moisture|soil_water_content|carbon_percent|nitrogen_percent|atmospheric_humidity|temperature|  pH|\n",
      "+---------+-------------+------------------+--------------+----------------+--------------------+-----------+----+\n",
      "|       58|        89.17|              9.45|          0.65|            3.69|               47.08|       36.3|4.85|\n",
      "|       81|         82.7|             57.07|          4.34|            4.44|               10.99|       3.85|4.58|\n",
      "|      124|         82.3|             16.52|          6.78|            0.67|               78.89|      22.29|4.53|\n",
      "|      162|        88.89|               7.8|          8.01|            2.59|               79.28|      20.31|4.86|\n",
      "|      170|        82.53|             55.56|          7.48|            1.82|               57.71|      31.78|4.85|\n",
      "+---------+-------------+------------------+--------------+----------------+--------------------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Multi-Column Sorting & Filtering.\n",
    "print(\"Task 3: Multi-Column Sorting & Filtering\")\n",
    "sorted_sample = df_clean.sample(fraction=0.0001, seed=42) \\\n",
    "                        .orderBy(\"timestamp\", \"soil_moisture\", \"temperature\")\n",
    "print(\"Sorted sample:\")\n",
    "sorted_sample.show(5)\n",
    "filtered_df = df_clean.filter((col(\"soil_moisture\") > 80) & (col(\"pH\") < 5))\n",
    "print(\"Filtered sample:\")\n",
    "filtered_df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4: Statistical Analysis\n",
      "Carbon Percent -> Mean: 5.00, Median: 5.00, StdDev: 2.89, Q1: 2.50, Q3: 7.50\n",
      "Nitrogen Percent -> Mean: 2.50, Median: 2.50, StdDev: 1.44, Q1: 1.25, Q3: 3.75\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Statistical Analysis.\n",
    "print(\"Task 4: Statistical Analysis\")\n",
    "stats = df_clean.select(\n",
    "    mean(\"carbon_percent\").alias(\"carbon_mean\"),\n",
    "    percentile_approx(\"carbon_percent\", 0.5, 1000).alias(\"carbon_median\"),\n",
    "    stddev(\"carbon_percent\").alias(\"carbon_stddev\"),\n",
    "    percentile_approx(\"carbon_percent\", [0.25, 0.75], 1000).alias(\"carbon_quartiles\"),\n",
    "    mean(\"nitrogen_percent\").alias(\"nitrogen_mean\"),\n",
    "    percentile_approx(\"nitrogen_percent\", 0.5, 1000).alias(\"nitrogen_median\"),\n",
    "    stddev(\"nitrogen_percent\").alias(\"nitrogen_stddev\"),\n",
    "    percentile_approx(\"nitrogen_percent\", [0.25, 0.75], 1000).alias(\"nitrogen_quartiles\")\n",
    ").collect()[0]\n",
    "print(f\"Carbon Percent -> Mean: {stats['carbon_mean']:.2f}, Median: {stats['carbon_median']:.2f}, StdDev: {stats['carbon_stddev']:.2f}, \" +\n",
    "      f\"Q1: {stats['carbon_quartiles'][0]:.2f}, Q3: {stats['carbon_quartiles'][1]:.2f}\")\n",
    "print(f\"Nitrogen Percent -> Mean: {stats['nitrogen_mean']:.2f}, Median: {stats['nitrogen_median']:.2f}, StdDev: {stats['nitrogen_stddev']:.2f}, \" +\n",
    "      f\"Q1: {stats['nitrogen_quartiles'][0]:.2f}, Q3: {stats['nitrogen_quartiles'][1]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5: Interpolation & Prediction\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Task 5: Interpolation & Prediction.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask 5: Interpolation & Prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df_interp \u001b[38;5;241m=\u001b[39m \u001b[43mdf_clean\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcarbon_percent_interp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m                     when(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcarbon_percent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNull(), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoil_moisture\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m      5\u001b[0m                     \u001b[38;5;241m.\u001b[39motherwise(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcarbon_percent\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      6\u001b[0m                 )\n\u001b[0;32m      7\u001b[0m sample_pred \u001b[38;5;241m=\u001b[39m df_interp\u001b[38;5;241m.\u001b[39msample(fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      8\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoil_moisture\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# Task 5: Interpolation & Prediction.\n",
    "print(\"Task 5: Interpolation & Prediction\")\n",
    "df_interp = df_clean.withColumn(\"carbon_percent_interp\",\n",
    "                    when(col(\"carbon_percent\").isNull(), col(\"soil_moisture\") * 0.05)\n",
    "                    .otherwise(col(\"carbon_percent\"))\n",
    "                )\n",
    "sample_pred = df_interp.sample(fraction=0.001, seed=42)\n",
    "assembler = VectorAssembler(inputCols=[\"soil_moisture\", \"temperature\"], outputCol=\"features\")\n",
    "df_vector = assembler.transform(sample_pred)\n",
    "train_df, test_df = df_vector.randomSplit([0.8, 0.2], seed=42)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"carbon_percent_interp\")\n",
    "lr_model = lr.fit(train_df)\n",
    "predictions = lr_model.transform(test_df)\n",
    "print(\"Prediction sample:\")\n",
    "predictions.select(\"timestamp\", \"soil_moisture\", \"temperature\", \"carbon_percent_interp\", \"prediction\").show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 6: Generating Visualization\n",
      "Bar chart saved as stats_plot.png\n"
     ]
    }
   ],
   "source": [
    "def plot_stats(stats, output_path):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Create a DataFrame with the statistics.\n",
    "    stats_pd = pd.DataFrame({\n",
    "        \"Stat\": [\"Mean\", \"Median\", \"StdDev\", \"Q1\", \"Q3\"],\n",
    "        \"Carbon\": [stats[\"carbon_mean\"], stats[\"carbon_median\"], stats[\"carbon_stddev\"],\n",
    "                   stats[\"carbon_quartiles\"][0], stats[\"carbon_quartiles\"][1]],\n",
    "        \"Nitrogen\": [stats[\"nitrogen_mean\"], stats[\"nitrogen_median\"], stats[\"nitrogen_stddev\"],\n",
    "                     stats[\"nitrogen_quartiles\"][0], stats[\"nitrogen_quartiles\"][1]]\n",
    "    })\n",
    "    # Plot the DataFrame as a bar chart.\n",
    "    ax = stats_pd.plot(x=\"Stat\", y=[\"Carbon\", \"Nitrogen\"], kind=\"bar\", figsize=(10,6),\n",
    "                         title=\"Soil Nutrient Statistics\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "# Task 6: Visualization (using a separate thread).\n",
    "print(\"Task 6: Generating Visualization\")\n",
    "plot_thread = Thread(target=plot_stats, args=(stats, \"stats_plot.png\"))\n",
    "plot_thread.start()\n",
    "plot_thread.join()\n",
    "print(\"Bar chart saved as stats_plot.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Runtime: 2366.05 seconds\n",
      "Total Runtime: 0.17 seconds\n"
     ]
    }
   ],
   "source": [
    "# End Timing & Report Performance.\n",
    "runtime = time.time() - start_time\n",
    "print(f\"Total Runtime: {runtime:.2f} seconds\")\n",
    "spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
